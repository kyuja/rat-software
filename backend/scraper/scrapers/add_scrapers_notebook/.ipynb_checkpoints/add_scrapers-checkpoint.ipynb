{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3663bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium\n",
    "#!pip install selenium-wire\n",
    "#!pip install beautifulsoup4\n",
    "#!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a420a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraper class to initialize standard features\n",
    "\n",
    "import json\n",
    "import base64\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from urllib import request\n",
    "from urllib.parse import urlsplit\n",
    "from urllib.parse import urlparse\n",
    "import urllib.parse\n",
    "import socket\n",
    "\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "import uuid #used to generate random file names\n",
    "\n",
    "import time\n",
    "\n",
    "class Scraping:\n",
    "\n",
    "    def __init__(self):\n",
    "        self = self\n",
    "\n",
    "    def __del__(self):\n",
    "        print('Helper object destroyed')\n",
    "\n",
    "    def encode_code(self, code):\n",
    "        code = code.encode('utf-8','ignore')\n",
    "        code = base64.b64encode(code)\n",
    "        return code\n",
    "\n",
    "    def decode_code(self, value):\n",
    "\n",
    "\n",
    "        try:\n",
    "            code_decoded = base64.b64decode(value)\n",
    "            code_decoded = BeautifulSoup(code_decoded, \"html.parser\")\n",
    "            code_decoded = str(code_decoded)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            code_decoded = \"decoding error\"\n",
    "        return code_decoded\n",
    "\n",
    "\n",
    "\n",
    "    def decode_picture(self, value):\n",
    "        picture = value.tobytes()\n",
    "        picture = picture.decode('ascii')\n",
    "        return picture\n",
    "\n",
    "    def get_result_meta(self, url):\n",
    "        meta = {}\n",
    "        ip = \"-1\"\n",
    "        main = url\n",
    "        #parse url to get hostname and socket\n",
    "        try:\n",
    "            parsed_uri = urlparse(url)\n",
    "            hostname = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "            ip = socket.gethostbyname(hostname)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            ip = \"-1\"\n",
    "\n",
    "        try:\n",
    "            main = '{0.scheme}://{0.netloc}/'.format(urlsplit(url))\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            main = url\n",
    "\n",
    "        #write to meta dictionary\n",
    "        meta = {\"ip\":ip, \"main\":main}\n",
    "\n",
    "        return meta\n",
    "\n",
    "    def get_chrome_extension(self):\n",
    "        #change extension path to load the I-don't care about cookies file\n",
    "        \n",
    "        currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "        parentdir = os.path.dirname(currentdir)\n",
    "        \n",
    "        #check for the used operating system to load the correct folder structure\n",
    "        if os.name == \"nt\":\n",
    "            extension_path = r'C:\\Users\\stahl\\Documents\\GitHub\\rat\\rat-backend\\scraper\\scrapers\\add_scrapers_notebook\\I-don-t-care-about-cookies.crx'\n",
    "        else:\n",
    "            extension_path = '//I-don-t-care-about-cookies.crx'\n",
    "        return extension_path\n",
    "\n",
    "\n",
    "    def take_screenshot(self, driver):\n",
    "\n",
    "        #function to encode file content to base64\n",
    "        def encode_file_base64(self, file):\n",
    "            f = open(file, 'rb')\n",
    "            code = f.read()\n",
    "            code = base64.b64encode(code)\n",
    "            f.close()\n",
    "            return code\n",
    "\n",
    "        current_path = os.path.abspath(os.getcwd())\n",
    "\n",
    "        #iniatilize constant variables\n",
    "\n",
    "        #iniatilize the directories for the extension and for the folder for temporary downlods of files\n",
    "        \n",
    "        #check for the used operating system to load the correct folder structure\n",
    "        if os.name == \"nt\":\n",
    "            screenshot_folder = r'C:\\Users\\stahl\\Documents\\GitHub\\rat\\rat-backend\\scraper\\scrapers\\add_scrapers_notebook\\tmp\\\\'\n",
    "\n",
    "\n",
    "        else:\n",
    "            screenshot_folder = \"//tmp//\"\n",
    "\n",
    "        screenshot_file = screenshot_folder+str(uuid.uuid1())+\".png\"\n",
    "              \n",
    "        time.sleep(2)\n",
    "\n",
    "        driver.maximize_window() #maximize browser window for screenshot\n",
    "        \n",
    "        try:\n",
    "            driver.execute_script(\"window.scrollTo(0,1)\")\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            pass\n",
    "\n",
    "        #try to get the whole browser window\n",
    "        try: \n",
    "            required_width = driver.execute_script('return document.body.parentNode.scrollWidth')\n",
    "            required_height = driver.execute_script('return document.body.parentNode.scrollHeight')\n",
    "                      \n",
    "            scroll = \"window.scrollTo(0,{})\".format(required_height)\n",
    "            \n",
    "            driver.execute_script(scroll)\n",
    "\n",
    "            required_height+= 50\n",
    "            \n",
    "            driver.execute_script(\"window.scrollTo(0,1)\")\n",
    "            \n",
    "            driver.set_window_size(required_width, required_height)\n",
    "            \n",
    "            driver.maximize_window()\n",
    "            \n",
    "            driver.save_screenshot(screenshot_file) #take screenshot\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(str(e)) ##next try based on the document.body\n",
    "\n",
    "            try: \n",
    "                body_screenshot = driver.find_element(By.TAG_NAME, \"body\")\n",
    "                body_screenshot.screenshot(screenshot_file)\n",
    "            except Exception as e:\n",
    "                print(str(e)) #if all fails take screenshot of the browser view\n",
    "                driver.save_screenshot(screenshot_file) #take screenshot\n",
    "\n",
    "        #open screenshot and save as base64\n",
    "        screenshot = encode_file_base64(self, screenshot_file)\n",
    "\n",
    "        #os.remove(screenshot_file) #remove the comment if you don't want to store the screenshots\n",
    "\n",
    "        return screenshot #return base64 code of image\n",
    "\n",
    "    #function to read out redirected urls\n",
    "\n",
    "    def get_real_url(url, driver):\n",
    "                \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(4)\n",
    "            current_url = driver.current_url #read real url (redirected url)\n",
    "            driver.quit()\n",
    "            return current_url\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dcc3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This template describes the steps necessary to add a custom scraper for the RAT software. First of all, it is assumed that these are search services that provide search forms. However, it is also possible to add other search systems. For this, the procedure would have to be adapted accordingly. Selenium is used as the basis for scraping.\n",
    "\n",
    "For scraping search results it is necessary to develop the name of the search service to be scraped as well as a scraper Python file for it. In this file appropriate functions must be defined, which are identical for all search engines. However, the contents can also be designed very individually. It is important that at the end search results with the following contents are returned, whereby fields can be filled also empty or with placeholders:\n",
    "\n",
    "- result_title: title in the snippet of a result\n",
    "- result_description: description in the snippet of a result\n",
    "- result_url: url of the search result\n",
    "- serp_code: html source of the search result page, if it is used for other analyses\n",
    "- serp_bin: screenshot of the search result page, if it is needed for other analyses\n",
    "- page: most search engines offer the possibility to browse the result pages. This variable is needed to scroll further. However, for search services where new search results are added by scrolling, it must be adjusted accordingly.\n",
    "\n",
    "Overall, a scraper usually consists of the following functions:\n",
    "- run(query, limit, scraping): main function for all scrapers with the following parameters: query = search query, limit = maximum number of results to query, scraping = scraping object with functions to scrape the search engines\n",
    "- get_search_results(driver, page): subfunction to retrieve the search results with the following parameters:\n",
    "driver = selenium driver; webbrowser for scraping; page = SERP page\n",
    "- check_captcha(driver): Helper function to check if there is a lockout of the search services. The function is called to indicate that scraping has been aborted. If checking it is False, the scraping will continue.\n",
    "\n",
    "The following will describe all standard variables and functionality to scrape a search engine. But again, it is also possible to change it all accordingly to the search engine to be scraped.\n",
    "\n",
    "Link to the documentation on locating elements in Selenium: https://selenium-python.readthedocs.io/locating-elements.html\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#required libs for web scraping\n",
    "\n",
    "#import external libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException #used to interrupt loding of websites and needed as workaround to download files with selenium\n",
    "from selenium.webdriver.common.action_chains import ActionChains #used to simulate pressing of a key\n",
    "\n",
    "import uuid #used to generate random file names\n",
    "\n",
    "import time #used to do timeout breaks\n",
    "\n",
    "import os #used for file management\n",
    "\n",
    "#base64 encoding to convert the code codes of webpages\n",
    "import base64\n",
    "\n",
    "#BeautifulSoup is necessary to beautify the code coded after it has been decoded (especially useful to prevent character errors)\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "\n",
    "import random\n",
    "\n",
    "#main function to run a scraper\n",
    "\n",
    "def run(query, limit, scraping, headless):\n",
    "    try:\n",
    "        #Definition of args for scraping the search engine\n",
    "        search_url = \"https://katalogplus.sub.uni-hamburg.de/vufind/Search2/\" #URL of search engine, e. g. www.google.de\n",
    "        search_box = \"lookfor\" #Class name of search box; input field for searches\n",
    "        captcha = \"g-recaptcha\" #Source code hint for CAPTCHA; some search engines use CAPTCHAS to block too many automatic requests\n",
    "        next_page = \"//a[@class='page-next']\" #CSS to find click on next SERP; for search engines that use a navigation on SERPS to continue browsing more search results\n",
    "        results_number = 0 #initialize results_number; normally starts at 0\n",
    "        page = 1 #initialize SERP page; normally starts at 1\n",
    "        search_results = [] #initialize search_results list\n",
    "\n",
    "        #Definition of custom functions\n",
    "\n",
    "        #Function to scrape search results\n",
    "        def get_search_results(driver, page):\n",
    "\n",
    "            get_search_results = [] #temporary list to store search results\n",
    "\n",
    "            source = driver.page_source #storing the source code of a search engine result page\n",
    "\n",
    "            serp_code = scraping.encode_code(source) #use the function from the lib scraping to encode the source code to base64; preparing it to be stored in database\n",
    "\n",
    "            serp_bin = scraping.take_screenshot(driver) #use the function to take a screenshot, encoding it to base64, to store it in database\n",
    "\n",
    "            soup = BeautifulSoup(source, features=\"lxml\") #use BeautifulSoup to read the source code of search engine result page and prepare it to read the tree to extract the search results\n",
    "\n",
    "            #Procedure to read out the search results with title, description and URL. There are several ways to extract such information. In this example we will use functions from BeautifulSoup to read out content of Div containers. Other approaches could be using Selenium or XPATH. Choosing the best way always depends on the search engine.\n",
    "            \n",
    "            #first find each block with one result\n",
    "            for result in soup.find_all(\"div\", class_=[\"result-body\"]):\n",
    "\n",
    "                url_list = []\n",
    "\n",
    "                result_description = \"N/A\"\n",
    "                result_url = \"N/A\"\n",
    "                search_result = []\n",
    "                result_title = \"\"\n",
    "                result_description = \"\"\n",
    "                \n",
    "                #try to extract the title based on a css class for the link title\n",
    "                try:\n",
    "                    for title in result.find(\"a\", class_=[\"title getFull\"]):\n",
    "                        result_title=title.text.strip()\n",
    "                except:\n",
    "                    result_title = \"N/A\"\n",
    "\n",
    "                try:\n",
    "                    #try to extract the result description by css class and change the content of the description, if necessary:\n",
    "\n",
    "                    for description in result.find_all(\"div\"):\n",
    "                        result_description+=description.text.strip()\n",
    "                        result_description = result_description.replace(result_title, \" \")\n",
    "                        result_description = result_description.replace(\"\\n\", \"\")\n",
    "                        result_description = result_description.replace(\"Veröffentlicht\", \" Veröffentlicht\")\n",
    "                        result_description = \" \".join(result_description.split())\n",
    "                except Exception as e:\n",
    "                    result_description = \"N/A\"\n",
    "                    \n",
    "                #try to extract the urls of the seaech results and change the url if the search engines works with relative hyperlinks\n",
    "                try:\n",
    "                    for url in result.find_all(\"a\"):\n",
    "                        url = url.attrs['href']\n",
    "                        url_list.append(url)\n",
    "                        result_url = url_list[0]\n",
    "                        result_url = \"https://katalogplus.sub.uni-hamburg.de\"+result_url\n",
    "                except:\n",
    "                    result_url = \"N/A\"\n",
    "\n",
    "                get_search_results.append([result_title, result_description, result_url, serp_code, serp_bin, page]) #call function to read search results and append the results to the list\n",
    "\n",
    "            return get_search_results\n",
    "\n",
    "        #Function to check if search engine shows CAPTCHA code. some search engines notice that a bot is trying to scrape their results. if that happens they tend to show a message or captcha\n",
    "        def check_captcha(driver):\n",
    "            source = driver.page_source\n",
    "            if captcha in source:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        chrome_extension = scraping.get_chrome_extension() #Get Path for I don't care about cookies extension\n",
    "\n",
    "        #initialize Selenium with options and arguments. Chrome supports a lot of arguments, a comprehensive list can be found here: https://peter.sh/experiments/chromium-command-line-switches/\n",
    "        options = Options()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        if headless == 1: #argument to check if browser should be started in headless mode or not. 1 = without browser head; 0 = with browser head for debuggin\n",
    "            options.add_argument('--headless=new')\n",
    "        options.add_argument(\"--start-maximized\")\n",
    "        options.add_experimental_option(\"detach\", True)\n",
    "        options.add_argument(\"--lang=de\")\n",
    "        options.add_extension(chrome_extension)\n",
    "        \n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.set_page_load_timeout(20)\n",
    "        driver.implicitly_wait(30)\n",
    "        driver.get(search_url)\n",
    "        random_sleep = random.randint(2, 5) #random timer trying to prevent quick automatic blocking\n",
    "        time.sleep(random_sleep)\n",
    "        \n",
    "\n",
    "        #Start scraping if no CAPTCHA; not necessary if search engine doesn't use mechanism to block automatic requests\n",
    "        if not check_captcha(driver):\n",
    "\n",
    "\n",
    "            #commands to trigger a search; some search engines just use URL GET-pararmeters which could be passed too\n",
    "            search = driver.find_element(By.NAME, search_box)\n",
    "            search.send_keys(query)\n",
    "            search.send_keys(Keys.RETURN)\n",
    "\n",
    "            search_results = get_search_results(driver, page)\n",
    "            results_number = len(search_results)\n",
    "\n",
    "            if headless == 1:\n",
    "                driver.quit()\n",
    "            return search_results\n",
    "\n",
    "\n",
    "        else:\n",
    "            search_results = -1\n",
    "            if headless == 1:\n",
    "                driver.quit()\n",
    "            return search_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        search_results = -1\n",
    "        return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27cd7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test the scraper. it shows all scraped details or an error message, if it fails\n",
    "\n",
    "def test_scraper(query, limit, scraper, headless):\n",
    "    search_results = run(query, limit, scraper, headless)\n",
    "\n",
    "    i = 0\n",
    "    if search_results != -1:\n",
    "        for sr in search_results:\n",
    "            i+=1\n",
    "            print(i)\n",
    "            print(sr[0])\n",
    "            print(sr[1])\n",
    "            print(sr[2])\n",
    "    else:\n",
    "        print(\"Scraping failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12bc2ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the scraper\n",
    "scraper = Scraping() #initialize the scraping object\n",
    "query = \"politik\" #search query\n",
    "limit = 10 #max_number of results (the scraper normally adds some more pages since not all search engines deliver a certain number of search results on every SERP)\n",
    "headless = 0 #0 = don't start selenium in headless mode (good for debugging); 1 = start selenium in headless mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0389e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3236\n",
      "1\n",
      "Nicht nur der Kernhaushalt\n",
      "(2022), 261 vom: 9. Nov., Seite 8von Bingener, Reinhardenthalten in: Frankfurter Allgemeine / D\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1853818542?rank=1\n",
      "2\n",
      "Gruß aus der Konzeptküche\n",
      "Mai/Juni 2023, Nr 3, 78. Jahr, Seite 38-47von Holdinghausen, Heikeenthalten in: Internationale Politik\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1844910210?rank=2\n",
      "3\n",
      "Liberaler Internationalismus\n",
      "März/April 2023, Nr 2, 78. Jahr, Seite 100-105von Schmid, Thomasenthalten in: Internationale Politik\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1838766340?rank=3\n",
      "4\n",
      "Die problematischen Partner\n",
      "September/Oktober 2023, Nr 5, 78. Jahr, Seite 18-23von Stent, Angelaenthalten in: Internationale Politik\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1858339391?rank=4\n",
      "5\n",
      "Insiderhandel in der Politik            : Status quo und Reformvorschläge\n",
      "23(2023), 4 vom: 19. Apr., Seite 234-238von Saathoff, Jonasenthalten in: Zeitschrift für Bank- und Kapitalmarktrecht\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1844702774?rank=5\n",
      "6\n",
      "Von Bremsern und Bürokraten\n",
      "(2023), 2, Seite 14-19von Matlé, Aylinenthalten in: Internationale Politik. Special\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1838781269?rank=6\n",
      "7\n",
      "How Germany lost the trust of Eastern Europe\n",
      "(2023), 1 vom: 4. Jan.von Schmelter, Lukas Paulenthalten in: Internationale Politik quarterly\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1841204625?rank=7\n",
      "8\n",
      "Angstkommunikation als Medium paternalistischer Krisenbewältigungspolitik am Beispiel der Corona-Pandemie\n",
      "70(2023), 2, Seite 130-154von Bach, Maurizioenthalten in: Zeitschrift für Politik\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1851417001?rank=8\n",
      "9\n",
      "„Es braucht eine Politik, die auf Wissenstransfer und Kooperation setzt“\n",
      "Mai/Juni 2023, Nr 3, 78. Jahr, Seite 25-29von Özdemir, Cementhalten in: Internationale Politik\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1844821668?rank=9\n",
      "10\n",
      "\"In Deutschland fehlt eine Industrie-Politik\"            : Rino Brugge, CEO der Kieler Werft German Naval Yards, spricht über Chancen in der \"Zeitenwende\" und der Energiewende, das Orderbuch, Offshore-Plattformen, den deutschen Werft-\"Flickenteppich\", den Eigner und dessen Wunsch nach mehr Kooperation\n",
      "160(2023), 5 vom: Mai, Seite 34-37von Meyer, Michaelenthalten in: Hansa\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1846258324?rank=10\n",
      "11\n",
      "Globaler Inflationsdruck            : Ursachen und länderspezifische Unterschiede\n",
      "73(2023), 1/3, Seite 28-35von Schnabl, Guntherenthalten in: Aus Politik und Zeitgeschichte\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1835738478?rank=11\n",
      "12\n",
      "Name it, count it, end it. Femizide erkennen, erfassen und beenden\n",
      "73(2023), 14, Seite 16-22von Sauer, Birgitenthalten in: Aus Politik und Zeitgeschichte\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1842078321?rank=12\n",
      "13\n",
      "Hase und  Igel im Darknet            : Computerwürmer, kriminelle Banden und ihre Widersacher\n",
      "73(2023), 22-24, Seite 4-9von Wolfangel, Evaenthalten in: Aus Politik und Zeitgeschichte\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1850912319?rank=13\n",
      "14\n",
      "Name it, count it, end it            : Femizide erkennen, erfassen und beenden\n",
      "73(2023), 14, Seite 16-22von Sauer, Birgitenthalten in: Aus Politik und Zeitgeschichte\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1843297175?rank=14\n",
      "15\n",
      "Gewalt gegen Frauen in den Nachrichten\n",
      "73(2023), 14, Seite 29-34von Meltzer, Christine E.enthalten in: Aus Politik und Zeitgeschichte\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1843296691?rank=15\n",
      "16\n",
      "\"Von der Verlobung zur Rivalität\"            : China und seine benachbarten Großmächte\n",
      "73(2023), 26/27, Seite 12-17von Gu, Xuewuenthalten in: Aus Politik und Zeitgeschichte\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1856819078?rank=16\n",
      "17\n",
      "Eine Welt ohne Hunger bis 2030?            : Stand und Perspektiven für das Sustainable Development Goal 2\n",
      "73(2023), 30/32, Seite 20-26von Quaim, Matinenthalten in: Aus Politik und Zeitgeschichte\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1857897277?rank=17\n",
      "18\n",
      "Einführung in das Welternährungssystem\n",
      "73(2023), 30/32, Seite 4-12von Brüntrup, Michaelenthalten in: Aus Politik und Zeitgeschichte\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1854202995?rank=18\n",
      "19\n",
      "Relasi Kuasa dan Suara: Politik Patron Klien Pada Pilkada Langsung di Kabupaten Grobogan 2020\n",
      "13(2022), 1, Seite 167-184von Muhamad Nastainin: Politika: Jurnal Ilmu Politik\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/DOAJ043391729?rank=19\n",
      "20\n",
      "Generation Gerechtigkeit\n",
      "März/April 2022, Nr. 2, 77. Jahr, Seite 9-11von Boddenberg, Sophiaenthalten in: Internationale Politik\n",
      "https://katalogplus.sub.uni-hamburg.de/vufind/Search2Record/1794497455?rank=20\n"
     ]
    }
   ],
   "source": [
    "test_scraper(query, limit, scraper, headless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263056d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
