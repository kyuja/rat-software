{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3663bd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (4.12.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium) (0.10.4)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: selenium-wire in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: blinker>=1.4 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium-wire) (1.6.2)\n",
      "Requirement already satisfied: brotli>=1.0.9 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium-wire) (1.1.0)\n",
      "Requirement already satisfied: certifi>=2019.9.11 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium-wire) (2023.7.22)\n",
      "Requirement already satisfied: kaitaistruct>=0.7 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium-wire) (0.10)\n",
      "Requirement already satisfied: pyasn1>=0.3.1 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium-wire) (0.4.8)\n",
      "Requirement already satisfied: pyOpenSSL>=22.0.0 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium-wire) (23.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.4.2 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium-wire) (3.0.9)\n",
      "Requirement already satisfied: pysocks>=1.7.1 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium-wire) (1.7.1)\n",
      "Requirement already satisfied: selenium>=4.0.0 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium-wire) (4.12.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium-wire) (1.2.0)\n",
      "Requirement already satisfied: zstandard>=0.14.1 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium-wire) (0.19.0)\n",
      "Requirement already satisfied: h2>=4.0 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium-wire) (4.1.0)\n",
      "Requirement already satisfied: hyperframe>=6.0 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium-wire) (6.0.1)\n",
      "Requirement already satisfied: pydivert>=2.0.3 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium-wire) (2.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from h2>=4.0->selenium-wire) (4.0.0)\n",
      "Requirement already satisfied: cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from pyOpenSSL>=22.0.0->selenium-wire) (41.0.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium>=4.0.0->selenium-wire) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium>=4.0.0->selenium-wire) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from selenium>=4.0.0->selenium-wire) (0.10.4)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from wsproto>=0.14->selenium-wire) (0.14.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0->pyOpenSSL>=22.0.0->selenium-wire) (1.15.1)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.0.0->selenium-wire) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.0.0->selenium-wire) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.0.0->selenium-wire) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.0.0->selenium-wire) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.0.0->selenium-wire) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.0.0->selenium-wire) (1.1.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0->pyOpenSSL>=22.0.0->selenium-wire) (2.21)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: lxml in c:\\users\\hilfskraft\\anaconda3\\lib\\site-packages (4.9.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install selenium-wire\n",
    "!pip install beautifulsoup4\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a420a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraper class to initialize standard features\n",
    "\n",
    "import json\n",
    "import base64\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from urllib import request\n",
    "from urllib.parse import urlsplit\n",
    "from urllib.parse import urlparse\n",
    "import urllib.parse\n",
    "import socket\n",
    "\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "import uuid #used to generate random file names\n",
    "\n",
    "import time\n",
    "\n",
    "class Scraping:\n",
    "\n",
    "    def __init__(self):\n",
    "        self = self\n",
    "\n",
    "    def __del__(self):\n",
    "        print('Helper object destroyed')\n",
    "\n",
    "    def encode_code(self, code):\n",
    "        code = code.encode('utf-8','ignore')\n",
    "        code = base64.b64encode(code)\n",
    "        return code\n",
    "\n",
    "    def decode_code(self, value):\n",
    "\n",
    "\n",
    "        try:\n",
    "            code_decoded = base64.b64decode(value)\n",
    "            code_decoded = BeautifulSoup(code_decoded, \"html.parser\")\n",
    "            code_decoded = str(code_decoded)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            code_decoded = \"decoding error\"\n",
    "        return code_decoded\n",
    "\n",
    "\n",
    "\n",
    "    def decode_picture(self, value):\n",
    "        picture = value.tobytes()\n",
    "        picture = picture.decode('ascii')\n",
    "        return picture\n",
    "\n",
    "    def get_result_meta(self, url):\n",
    "        meta = {}\n",
    "        ip = \"-1\"\n",
    "        main = url\n",
    "        #parse url to get hostname and socket\n",
    "        try:\n",
    "            parsed_uri = urlparse(url)\n",
    "            hostname = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "            ip = socket.gethostbyname(hostname)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            ip = \"-1\"\n",
    "\n",
    "        try:\n",
    "            main = '{0.scheme}://{0.netloc}/'.format(urlsplit(url))\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            main = url\n",
    "\n",
    "        #write to meta dictionary\n",
    "        meta = {\"ip\":ip, \"main\":main}\n",
    "\n",
    "        return meta\n",
    "\n",
    "    def get_chrome_extension(self):\n",
    "        #change extension path to load the I-don't care about cookies file\n",
    "        \n",
    "        currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "        parentdir = os.path.dirname(currentdir)\n",
    "        \n",
    "        #check for the used operating system to load the correct folder structure\n",
    "        if os.name == \"nt\":\n",
    "            extension_path = r'C:\\Users\\Hilfskraft\\Documents\\Scraper\\I-don-t-care-about-cookies.crx'\n",
    "        else:\n",
    "            extension_path = '//I-don-t-care-about-cookies.crx'\n",
    "        return extension_path\n",
    "\n",
    "\n",
    "    def take_screenshot(self, driver):\n",
    "\n",
    "        #function to encode file content to base64\n",
    "        def encode_file_base64(self, file):\n",
    "            f = open(file, 'rb')\n",
    "            code = f.read()\n",
    "            code = base64.b64encode(code)\n",
    "            f.close()\n",
    "            return code\n",
    "\n",
    "        current_path = os.path.abspath(os.getcwd())\n",
    "\n",
    "        #iniatilize constant variables\n",
    "\n",
    "        #iniatilize the directories for the extension and for the folder for temporary downlods of files\n",
    "        \n",
    "        #check for the used operating system to load the correct folder structure\n",
    "        if os.name == \"nt\":\n",
    "            screenshot_folder = r'C:\\Users\\Hilfskraft\\Documents\\Scraper\\tmp'\n",
    "\n",
    "\n",
    "        else:\n",
    "            screenshot_folder = \"//tmp//\"\n",
    "\n",
    "        screenshot_file = screenshot_folder+str(uuid.uuid1())+\".png\"\n",
    "              \n",
    "        time.sleep(2)\n",
    "\n",
    "        driver.maximize_window() #maximize browser window for screenshot\n",
    "        \n",
    "        try:\n",
    "            driver.execute_script(\"window.scrollTo(0,1)\")\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            pass\n",
    "\n",
    "        #try to get the whole browser window\n",
    "        try: \n",
    "            required_width = driver.execute_script('return document.body.parentNode.scrollWidth')\n",
    "            required_height = driver.execute_script('return document.body.parentNode.scrollHeight')\n",
    "                      \n",
    "            scroll = \"window.scrollTo(0,{})\".format(required_height)\n",
    "            \n",
    "            driver.execute_script(scroll)\n",
    "\n",
    "            required_height+= 50\n",
    "            \n",
    "            driver.execute_script(\"window.scrollTo(0,1)\")\n",
    "            \n",
    "            driver.set_window_size(required_width, required_height)\n",
    "            \n",
    "            driver.maximize_window()\n",
    "            \n",
    "            driver.save_screenshot(screenshot_file) #take screenshot\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(str(e)) ##next try based on the document.body\n",
    "\n",
    "            try: \n",
    "                body_screenshot = driver.find_element(By.TAG_NAME, \"body\")\n",
    "                body_screenshot.screenshot(screenshot_file)\n",
    "            except Exception as e:\n",
    "                print(str(e)) #if all fails take screenshot of the browser view\n",
    "                driver.save_screenshot(screenshot_file) #take screenshot\n",
    "\n",
    "        #open screenshot and save as base64\n",
    "        screenshot = encode_file_base64(self, screenshot_file)\n",
    "\n",
    "        os.remove(screenshot_file) #remove the comment if you don't want to store the screenshots\n",
    "\n",
    "        return screenshot #return base64 code of image\n",
    "\n",
    "    #function to read out redirected urls\n",
    "\n",
    "    def get_real_url(url, driver):\n",
    "                \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(4)\n",
    "            current_url = driver.current_url #read real url (redirected url)\n",
    "            driver.quit()\n",
    "            return current_url\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dcc3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This template describes the steps necessary to add a custom scraper for the RAT software. First of all, it is assumed that these are search services that provide search forms. However, it is also possible to add other search systems. For this, the procedure would have to be adapted accordingly. Selenium is used as the basis for scraping.\n",
    "\n",
    "For scraping search results it is necessary to develop the name of the search service to be scraped as well as a scraper Python file for it. In this file appropriate functions must be defined, which are identical for all search engines. However, the contents can also be designed very individually. It is important that at the end search results with the following contents are returned, whereby fields can be filled also empty or with placeholders:\n",
    "\n",
    "- result_title: title in the snippet of a result\n",
    "- result_description: description in the snippet of a result\n",
    "- result_url: url of the search result\n",
    "- serp_code: html source of the search result page, if it is used for other analyses\n",
    "- serp_bin: screenshot of the search result page, if it is needed for other analyses\n",
    "- page: most search engines offer the possibility to browse the result pages. This variable is needed to scroll further. However, for search services where new search results are added by scrolling, it must be adjusted accordingly.\n",
    "\n",
    "Overall, a scraper usually consists of the following functions:\n",
    "- run(query, limit, scraping): main function for all scrapers with the following parameters: query = search query, limit = maximum number of results to query, scraping = scraping object with functions to scrape the search engines\n",
    "- get_search_results(driver, page): subfunction to retrieve the search results with the following parameters:\n",
    " driver = selenium driver; webbrowser for scraping; page = SERP page\n",
    "- check_captcha(driver): Helper function to check if there is a lockout of the search services. The function is called to indicate that scraping has been aborted. If checking it is False, the scraping will continue.\n",
    "\n",
    "The following will describe all standard variables and functionality to scrape a search engine. But again, it is also possible to change it all accordingly to the search engine to be scraped.\n",
    "\n",
    "Link to the documentation on locating elements in Selenium: https://selenium-python.readthedocs.io/locating-elements.html\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#required libs for web scraping\n",
    "\n",
    "#import external libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException #used to interrupt loding of websites and needed as workaround to download files with selenium\n",
    "from selenium.webdriver.common.action_chains import ActionChains #used to simulate pressing of a key\n",
    "\n",
    "import uuid #used to generate random file names\n",
    "\n",
    "import time #used to do timeout breaks\n",
    "\n",
    "import os #used for file management\n",
    "\n",
    "#base64 encoding to convert the code codes of webpages\n",
    "import base64\n",
    "\n",
    "#BeautifulSoup is necessary to beautify the code coded after it has been decoded (especially useful to prevent character errors)\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "\n",
    "import random\n",
    "\n",
    "#main function to run a scraper\n",
    "\n",
    "def run(query, limit, scraping, headless):\n",
    "    try:\n",
    "        #Definition of args for scraping the search engine\n",
    "        search_url = \"https://search.brave.com/\" #URL of search engine, e. g. www.google.de\n",
    "        search_box = \"q\" #Class name of search box; input field for searches\n",
    "        captcha = \"g-recaptcha\" #Source code hint for CAPTCHA; some search engines use CAPTCHAS to block too many automatic requests\n",
    "        next_page = \"//a[@class='btn svelte-o26zbb']\" #CSS to find click on next SERP; for search engines that use a navigation on SERPS to continue browsing more search results\n",
    "        results_number = 0 #initialize results_number; normally starts at 0\n",
    "        page = 1 #initialize SERP page; normally starts at 1\n",
    "        search_results = [] #initialize search_results list\n",
    "\n",
    "        #Definition of custom functions\n",
    "\n",
    "        #Function to scrape search results\n",
    "        def get_search_results(driver, page):\n",
    "            \n",
    "            counter = 0\n",
    "\n",
    "            get_search_results = [] #temporary list to store search results\n",
    "\n",
    "            source = driver.page_source #storing the source code of a search engine result page\n",
    "\n",
    "            serp_code = scraping.encode_code(source) #use the function from the lib scraping to encode the source code to base64; preparing it to be stored in database\n",
    "\n",
    "            serp_bin = scraping.take_screenshot(driver) #use the function to take a screenshot, encoding it to base64, to store it in database\n",
    "\n",
    "            soup = BeautifulSoup(source, features=\"lxml\") #use BeautifulSoup to read the source code of search engine result page and prepare it to read the tree to extract the search results\n",
    "\n",
    "            #Procedure to read out the search results with title, description and URL. There are several ways to extract such information. In this example we will use functions from BeautifulSoup to read out content of Div containers. Other approaches could be using Selenium or XPATH. Choosing the best way always depends on the search engine.\n",
    "            \n",
    "            #first find each block with one result\n",
    "            \n",
    "            for result in soup.find_all(\"div\", class_=[\"snippet svelte-uqu4ua\"]):\n",
    "                \n",
    "                url_list = []\n",
    "\n",
    "                result_description = \"N/A\"\n",
    "                result_url = \"N/A\"\n",
    "                search_result = []\n",
    "                result_title = \"\"\n",
    "                result_description = \"\"\n",
    "                \n",
    "                #try to extract the title based on a css class for the link title\n",
    "                try:\n",
    "                    for title in result.find(\"div\", class_=[\"title svelte-1mekd5e\"]):\n",
    "                        result_title=title.text.strip()\n",
    "                except:\n",
    "                    result_title = \"N/A\"\n",
    "\n",
    "                try:\n",
    "                    #try to extract the result description by css class and change the content of the description, if necessary:\n",
    "\n",
    "                    for description in result.find_all(\"div\", class_=[\"snippet-description\"]):\n",
    "                        result_description+=description.text.strip()\n",
    "                        result_description = result_description.replace(result_title, \" \")\n",
    "                        #result_description = result_description.replace(\"\\n\", \"\")\n",
    "                        result_description = \" \".join(result_description.split())\n",
    "                except Exception as e:\n",
    "                    result_description = \"N/A\"\n",
    "                    \n",
    "                #try to extract the urls of the seaech results and change the url if the search engines works with relative hyperlinks\n",
    "                try:\n",
    "                    for url in result.find_all(\"a\"):\n",
    "                        url = url.attrs['href']\n",
    "                        url_list.append(url)\n",
    "                        result_url = url_list[0]\n",
    "                except:\n",
    "                    result_url = \"N/A\"\n",
    "                    \n",
    "                if counter < limit+2:\n",
    "                    \n",
    "                    get_search_results.append([result_title, result_description, result_url, serp_code, serp_bin, page]) #call function to read search results and append the results to the list\n",
    "                    counter += 1\n",
    "                    \n",
    "            return get_search_results\n",
    "\n",
    "        #Function to check if search engine shows CAPTCHA code. some search engines notice that a bot is trying to scrape their results. if that happens they tend to show a message or captcha\n",
    "        def check_captcha(driver):\n",
    "            source = driver.page_source\n",
    "            if captcha in source:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        chrome_extension = scraping.get_chrome_extension() #Get Path for I don't care about cookies extension\n",
    "\n",
    "        #initialize Selenium with options and arguments. Chrome supports a lot of arguments, a comprehensive list can be found here: https://peter.sh/experiments/chromium-command-line-switches/\n",
    "        options = Options()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        if headless == 1: #argument to check if browser should be started in headless mode or not. 1 = without browser head; 0 = with browser head for debuggin\n",
    "            options.add_argument('--headless=new')\n",
    "        options.add_argument(\"--start-maximized\")\n",
    "        options.add_experimental_option(\"detach\", True)\n",
    "        options.add_argument(\"--lang=de\")\n",
    "        options.add_extension(chrome_extension)\n",
    "        \n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.set_page_load_timeout(20)\n",
    "        driver.implicitly_wait(30)\n",
    "        driver.get(search_url)\n",
    "        random_sleep = random.randint(2, 5) #random timer trying to prevent quick automatic blocking\n",
    "        time.sleep(random_sleep)\n",
    "        \n",
    "\n",
    "        #Start scraping if no CAPTCHA; not necessary if search engine doesn't use mechanism to block automatic requests\n",
    "        if not check_captcha(driver):\n",
    "\n",
    "\n",
    "            #commands to trigger a search; some search engines just use URL GET-pararmeters which could be passed too\n",
    "            search = driver.find_element(By.NAME, search_box)\n",
    "            search.send_keys(query)\n",
    "            search.send_keys(Keys.RETURN)\n",
    "\n",
    "            search_results = get_search_results(driver, page)\n",
    "            results_number = len(search_results)\n",
    "\n",
    "            if headless == 1:\n",
    "                driver.quit()\n",
    "            return search_results\n",
    "\n",
    "\n",
    "        else:\n",
    "            search_results = -1\n",
    "            if headless == 1:\n",
    "                driver.quit()\n",
    "            return search_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        search_results = -1\n",
    "        return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27cd7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test the scraper. it shows all scraped details or an error message, if it fails\n",
    "\n",
    "def test_scraper(query, limit, scraper, headless):\n",
    "    search_results = run(query, limit, scraper, headless)\n",
    "    \n",
    "    i = 0\n",
    "    if search_results != -1:\n",
    "        for sr in search_results:\n",
    "            i+=1\n",
    "            print(i)\n",
    "            print(sr[0])\n",
    "            print(sr[1])\n",
    "            print(sr[2]) \n",
    "    else:\n",
    "        print(\"Scraping failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12bc2ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the scraper\n",
    "scraper = Scraping() #initialize the scraping object\n",
    "query = \"manga\" #search query\n",
    "limit = 7 #max_number of results (the scraper normally adds some more pages since not all search engines deliver a certain number of search results on every SERP)\n",
    "headless = 0 #0 = don't start selenium in headless mode (good for debugging); 1 = start selenium in headless mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0389e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Manga – Wikipedia\n",
      "12. August 2002 - Manga [ˈmaŋɡa] (japanisch 漫画) ist der japanische Begriff für Comics. Außerhalb von Japan bezeichnet er meist ausschließlich aus Japan stammende Comics, wird aber auch für nichtjapanische Werke verwendet, die visuell und erzählerisch stark an japanische Vorbilder angelehnt sind.\n",
      "https://de.wikipedia.org/wiki/Manga\n",
      "2\n",
      "Manga Bücher online kaufen | Thalia\n",
      "26. April 2017 - Hier im Thalia-Onlineshop erwartet Sie ein vielfältiges Angebot an Mangas von Tokyopop, Egmont Manga und anderen bekannten Verlagen. Sie finden diverse humorvolle Mangas ebenso wie ernsthafte oder actionreiche. Die Bezeichnung Manga bezieht sich auf Comics, die aus Japan stammen.\n",
      "https://www.thalia.de/kategorie/manga-94/\n",
      "3\n",
      "Manga von Carlsen Manga! und Hayabusa | Carlsen\n",
      "31. Januar 2023 - Entdecke die Titel von Carlsen Manga!, darunter Bestseller wie NARUTO, MY HERO ACADEMIA, ATTACK ON TITAN , DRAGON BALL, ONE PIECE, FRUITS BASKET oder FAIRY TAIL.\n",
      "https://www.carlsen.de/manga\n",
      "4\n",
      "Lies Beliebte Manga Online - Crunchyroll\n",
      "24. November 2020 - Lies deine japanischen Lieblingsmanga online auf Crunchyroll, wie Attack on Titan, Fairy Tail, The Seven Deadly Sinds, Fuuka, Knights & Magic und viele andere mehr.\n",
      "https://www.crunchyroll.com/de/comics/manga\n",
      "5\n",
      "Manga Day 2023: Mehr als nur ein Phänomen aus Japan | NDR.de - ...\n",
      "vor 3 Tagen - Wenn es um Popkultur geht, liegen Eltern und ihre Kinder oft nicht auf der gleichen Wellenlänge. Beim Manga trifft das besonders zu. Dennoch lohnt sich der Griff zu den Heften, die man von rechts nach links liest.\n",
      "https://www.ndr.de/kultur/buch/Manga-Day-2023-Mehr-als-nur-ein-Phaenomen-aus-Japan,manga252.html\n",
      "6\n",
      "MANGA Plus by SHUEISHA\n",
      "vor 1 Woche - We're sorry but MANGA Plus doesn't work properly without JavaScript enabled. Please enable it to continue\n",
      "https://mangaplus.shueisha.co.jp/updates\n",
      "7\n",
      "Manga - Cross Cult - Comics & Romane\n",
      "27. August 2022 - Werde ein Teil des Kultes und sag uns deine Meinung in unserem Forum oder bei Facebook\n",
      "https://www.cross-cult.de/manga.html\n",
      "8\n",
      "Manga2You – Täglich aktuelle Manga-News aus Deutschland und Japan\n",
      "7. Januar 2017 - Manga2You versorgt dich jeden Tag mit den wichtigsten Manga-News aus Deutschland und Japan. Immer aktuell, schnell und auf Deutsch!\n",
      "https://www.manga2you.de/\n",
      "9\n",
      "Große Auswahl an Manga & Graphic Novel auf osiander.de | OSIANDER\n",
      "Buchhandlung OSIANDER - Bücher und eBooks online kaufen, versandkostenfrei liefern und loslesen. Über 70 Buchhandlungen vor Ort.\n",
      "https://www.osiander.de/kategorie/manga-94/\n"
     ]
    }
   ],
   "source": [
    "test_scraper(query, limit, scraper, headless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263056d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
