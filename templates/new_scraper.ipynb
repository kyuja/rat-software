{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3663bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium\n",
    "#!pip install selenium-wire\n",
    "#!pip install beautifulsoup4\n",
    "#!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a420a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraper class to initialize standard features\n",
    "\n",
    "#constants to load extensions and define the path for screenshots\n",
    "\n",
    "EXTENSION_PATH_NT = r'G:\\rat\\rat\\templates\\I-don-t-care-about-cookies.crx'\n",
    "EXTENSION_PATH_LINUX = '//I-don-t-care-about-cookies.crx'\n",
    "\n",
    "SCREENSHOT_PATH_NT = r'G:\\rat\\rat\\templates\\tmp\\\\'\n",
    "SCREENSHOT_PATH_LINUX = \"//tmp//\"\n",
    "\n",
    "import json\n",
    "import base64\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from urllib import request\n",
    "from urllib.parse import urlsplit\n",
    "from urllib.parse import urlparse\n",
    "import urllib.parse\n",
    "import socket\n",
    "\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "import uuid #used to generate random file names\n",
    "\n",
    "import time\n",
    "\n",
    "class Scraping:\n",
    "\n",
    "    def __init__(self):\n",
    "        self = self\n",
    "\n",
    "    def __del__(self):\n",
    "        print('Helper object destroyed')\n",
    "\n",
    "    def encode_code(self, code):\n",
    "        code = code.encode('utf-8','ignore')\n",
    "        code = base64.b64encode(code)\n",
    "        return code\n",
    "\n",
    "    def decode_code(self, value):\n",
    "\n",
    "        try:\n",
    "            code_decoded = base64.b64decode(value)\n",
    "            code_decoded = BeautifulSoup(code_decoded, \"html.parser\")\n",
    "            code_decoded = str(code_decoded)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            code_decoded = \"decoding error\"\n",
    "        return code_decoded\n",
    "\n",
    "\n",
    "\n",
    "    def decode_picture(self, value):\n",
    "        picture = value.tobytes()\n",
    "        picture = picture.decode('ascii')\n",
    "        return picture\n",
    "\n",
    "    def get_result_meta(self, url):\n",
    "        meta = {}\n",
    "        ip = \"-1\"\n",
    "        main = url\n",
    "        #parse url to get hostname and socket\n",
    "        try:\n",
    "            parsed_uri = urlparse(url)\n",
    "            hostname = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "            ip = socket.gethostbyname(hostname)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            ip = \"-1\"\n",
    "\n",
    "        try:\n",
    "            main = '{0.scheme}://{0.netloc}/'.format(urlsplit(url))\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            main = url\n",
    "\n",
    "        #write to meta dictionary\n",
    "        meta = {\"ip\":ip, \"main\":main}\n",
    "\n",
    "        return meta\n",
    "\n",
    "    def get_chrome_extension(self):\n",
    "        #change extension path to load the I-don't care about cookies file\n",
    "        \n",
    "        currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "        parentdir = os.path.dirname(currentdir)\n",
    "        \n",
    "        #check for the used operating system to load the correct folder structure\n",
    "        if os.name == \"nt\":\n",
    "            extension_path = EXTENSION_PATH_NT\n",
    "        else:\n",
    "            extension_path = EXTENSION_PATH_LINUX\n",
    "        return extension_path\n",
    "\n",
    "\n",
    "    def take_screenshot(self, driver):\n",
    "\n",
    "        #function to encode file content to base64\n",
    "        def encode_file_base64(self, file):\n",
    "            f = open(file, 'rb')\n",
    "            code = f.read()\n",
    "            code = base64.b64encode(code)\n",
    "            f.close()\n",
    "            return code\n",
    "\n",
    "        current_path = os.path.abspath(os.getcwd())\n",
    "\n",
    "        #iniatilize constant variables\n",
    "\n",
    "        #iniatilize the directories for the extension and for the folder for temporary downlods of files\n",
    "        \n",
    "        #check for the used operating system to load the correct folder structure\n",
    "        if os.name == \"nt\":\n",
    "            screenshot_folder = SCREENSHOT_PATH_NT\n",
    "\n",
    "\n",
    "        else:\n",
    "            screenshot_folder = SCREENSHOT_PATH_LINUX\n",
    "\n",
    "        screenshot_file = screenshot_folder+str(uuid.uuid1())+\".png\"\n",
    "              \n",
    "        time.sleep(2)\n",
    "\n",
    "        driver.maximize_window() #maximize browser window for screenshot\n",
    "        \n",
    "        try:\n",
    "            driver.execute_script(\"window.scrollTo(0,1)\")\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            pass\n",
    "\n",
    "        #try to get the whole browser window\n",
    "        try: \n",
    "            required_width = driver.execute_script('return document.body.parentNode.scrollWidth')\n",
    "            required_height = driver.execute_script('return document.body.parentNode.scrollHeight')\n",
    "                      \n",
    "            scroll = \"window.scrollTo(0,{})\".format(required_height)\n",
    "            \n",
    "            driver.execute_script(scroll)\n",
    "\n",
    "            required_height+= 50\n",
    "            \n",
    "            driver.execute_script(\"window.scrollTo(0,1)\")\n",
    "            \n",
    "            driver.set_window_size(required_width, required_height)\n",
    "            \n",
    "            driver.maximize_window()\n",
    "            \n",
    "            driver.save_screenshot(screenshot_file) #take screenshot\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(str(e)) ##next try based on the document.body\n",
    "\n",
    "            try: \n",
    "                body_screenshot = driver.find_element(By.TAG_NAME, \"body\")\n",
    "                body_screenshot.screenshot(screenshot_file)\n",
    "            except Exception as e:\n",
    "                print(str(e)) #if all fails take screenshot of the browser view\n",
    "                driver.save_screenshot(screenshot_file) #take screenshot\n",
    "\n",
    "        #open screenshot and save as base64\n",
    "        screenshot = encode_file_base64(self, screenshot_file)\n",
    "\n",
    "        #os.remove(screenshot_file) #remove the comment if you don't want to store the screenshots\n",
    "\n",
    "        return screenshot #return base64 code of image\n",
    "\n",
    "    #function to read out redirected urls\n",
    "\n",
    "    def get_real_url(url, driver):\n",
    "                \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(4)\n",
    "            current_url = driver.current_url #read real url (redirected url)\n",
    "            driver.quit()\n",
    "            return current_url\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dcc3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This template describes the steps required to add a custom scraper for the RAT software. First of all, it is assumed that these are search services that provide search forms. However, it is also possible to add other search systems. The procedure would have to be adapted accordingly. Selenium is used as the basis for scraping.\n",
    "\n",
    "A scraper generally consists of the following functions:\n",
    "- run(query, limit, scraping): Main function for all scrapers with the following parameters: query = search query, limit = maximum number of results to be retrieved, scraping = scraping object with functions for scraping the search engines\n",
    "- get_search_results(driver, page): Sub-function for retrieving the search results with the following parameters:\n",
    "driver = Selenium driver; web browser for scraping; page = SERP page\n",
    "- check_captcha(driver): Helper function to check whether there is a block on search services. The function is called to indicate that scraping has been cancelled. If the check is false, scraping is continued.\n",
    "\n",
    "All standard variables and functions for scraping a search engine are described below. However, it is also possible to change everything here according to the search engine to be scraped.\n",
    "\n",
    "Link to the documentation on finding elements on a webpage using Selenium: https://selenium-python.readthedocs.io/locating-elements.html\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#required libs for web scraping\n",
    "\n",
    "#import external libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException #used to interrupt loding of websites and needed as workaround to download files with selenium\n",
    "from selenium.webdriver.common.action_chains import ActionChains #used to simulate pressing of a key\n",
    "\n",
    "import uuid #used to generate random file names\n",
    "\n",
    "import time #used to do timeout breaks\n",
    "\n",
    "import os #used for file management\n",
    "\n",
    "#base64 encoding to convert the code codes of webpages\n",
    "import base64\n",
    "\n",
    "#BeautifulSoup is necessary to beautify the code coded after it has been decoded (especially useful to prevent character errors)\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "\n",
    "import random\n",
    "\n",
    "import re\n",
    "\n",
    "#main function to run a scraper\n",
    "\n",
    "def run(query, limit, scraping):\n",
    "    try:\n",
    "        #Definition of args for scraping the search engine\n",
    "        search_url = \"https://www.google.de\" #URL of search engine, e. g. www.google.de\n",
    "        search_box = \"q\" #Class name of search box; input field for searches\n",
    "        captcha = \"g-recaptcha\" #Source code hint for CAPTCHA; some search engines use CAPTCHAS to block too many automatic requests\n",
    "        next_page = \"//a[@aria-label='{}']\" #CSS to find click on next SERP; for search engines that use a navigation on SERPS to continue browsing more search results\n",
    "        results_number = 0 #initialize results_number; normally starts at 0\n",
    "        page = 1 #initialize SERP page; normally starts at 1\n",
    "        search_results = [] #initialize search_results list\n",
    "\n",
    "        #Definition of custom functions\n",
    "        \n",
    "        def search_pagination(source):\n",
    "            soup = BeautifulSoup(source, features=\"lxml\")\n",
    "            if soup.find(\"span\", class_=[\"SJajHc NVbCr\"]):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "        #Function to scrape search results\n",
    "        def get_search_results(driver, page):\n",
    "\n",
    "            get_search_results = []\n",
    "\n",
    "            source = driver.page_source\n",
    "\n",
    "            serp_code = scraping.encode_code(source)\n",
    "\n",
    "            serp_bin = scraping.take_screenshot(driver)\n",
    "\n",
    "            soup = BeautifulSoup(source, features=\"lxml\")\n",
    "            \n",
    "            #addtional steps to extract undesired elements from the Search Engine Result Page (SERP)\n",
    "\n",
    "            for s in soup.find_all(\"div\", class_=\"d4rhi\"):\n",
    "                s.extract()\n",
    "\n",
    "            for s in soup.find_all(\"div\", class_=\"Wt5Tfe\"):\n",
    "                s.extract()\n",
    "\n",
    "            for s in soup.find_all(\"div\", class_=\"UDZeY fAgajc OTFaAf\"):\n",
    "                s.extract()\n",
    "                \n",
    "            #find the list with the search results by extracting the div container\n",
    "\n",
    "            for result in soup.find_all(\"div\", class_=[\"tF2Cxc\"]):\n",
    "                url_list = []\n",
    "                search_result = []\n",
    "                result_title = \"\"\n",
    "                result_description = \"\"\n",
    "                result_url = \"\"\n",
    "                \n",
    "                #find result title of a search result by header class\n",
    "                try:\n",
    "                    for title in result.find(\"h3\", class_=[\"LC20lb MBeuO DKV0Md\"]):\n",
    "                        result_title+=title.text.strip()\n",
    "                except:\n",
    "                    result_title = \"N/A\"\n",
    "                    \n",
    "                \n",
    "                #find description of a search result by div container\n",
    "                try:                  \n",
    "                    for description in result.find(\"div\", class_=re.compile(\"VwiC3b\", re.I)):\n",
    "                       \n",
    "                        result_description+=description.text.strip()\n",
    "                except:\n",
    "                    result_description = \"N/A\"\n",
    "                    \n",
    "                #find url of a search result by href\n",
    "                try:\n",
    "                    for url in result.find_all(\"a\"):\n",
    "                        url = url.attrs['href']\n",
    "                        url_list.append(url)\n",
    "                        result_url = url_list[0]\n",
    "                except:\n",
    "                    result_url = \"N/A\"\n",
    "                \n",
    "                #add search results to list\n",
    "                get_search_results.append([result_title, result_description, result_url, serp_code, serp_bin, page])\n",
    "\n",
    "            return get_search_results\n",
    "\n",
    "        #Function to check if search engine shows CAPTCHA code\n",
    "        def check_captcha(driver):\n",
    "            source = driver.page_source\n",
    "            if captcha in source:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        chrome_extension = scraping.get_chrome_extension() #Get Path for I don't care about cookies extension\n",
    "\n",
    "        #initialize Selenium\n",
    "        options = Options()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument(\"--start-maximized\")\n",
    "        options.add_argument('--headless=new')\n",
    "        options.add_argument(\"--lang=de\")\n",
    "        options.add_experimental_option(\"detach\", True)\n",
    "        options.add_extension(chrome_extension)\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.set_page_load_timeout(20)\n",
    "        driver.implicitly_wait(30)\n",
    "        driver.get(search_url)\n",
    "        driver.maximize_window()\n",
    "        random_sleep = random.randint(2, 5)\n",
    "        time.sleep(random_sleep)\n",
    "\n",
    "    \n",
    "        #Start scraping if no CAPTCHA\n",
    "        if not check_captcha(driver):\n",
    "\n",
    "            search = driver.find_element(By.NAME, search_box)\n",
    "            search.send_keys(query)\n",
    "            search.send_keys(Keys.RETURN)\n",
    "\n",
    "            random_sleep = random.randint(2, 5)\n",
    "            time.sleep(random_sleep)\n",
    "\n",
    "            search_results = get_search_results(driver, page)\n",
    "\n",
    "            results_number = len(search_results)\n",
    "\n",
    "            continue_scraping = True\n",
    "\n",
    "            \"\"\"\n",
    "            Custom block to click on the next SERP page:\n",
    "            \n",
    "            Google offers two different ways to get more results on a SERP.  Either by clicking on a button for \"more results\" or by the classic scrolling on the SERP to subsequent pages. This customized block first checks which type of pagination is offered. \n",
    "            Subsequent search results are then scraped as long as the total number of search results is less than the defined limit.\n",
    "            \"\"\"\n",
    "            \n",
    "            check_pagination = search_pagination(source = driver.page_source)\n",
    "\n",
    "            if check_pagination: \n",
    "                #Click on next SERP pages as long the total number of results is lower the results limit\n",
    "                while (results_number < limit) and continue_scraping:\n",
    "                    if not check_captcha(driver):\n",
    "                        random_sleep = random.randint(2, 5)\n",
    "                        time.sleep(random_sleep)\n",
    "                        page+=1\n",
    "                        page_label = \"Page \"+str(page)\n",
    "                        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                        try:\n",
    "                            next = driver.find_element(By.XPATH, next_page.format(page_label))\n",
    "                            next.click()\n",
    "                            search_results+= get_search_results(driver, page)\n",
    "                            results_number = len(search_results)\n",
    "                        except:\n",
    "                            continue_scraping = False\n",
    "                    else:\n",
    "                        continue_scraping = False\n",
    "                        search_results = -1\n",
    "\n",
    "                driver.quit()\n",
    "\n",
    "                return search_results\n",
    "\n",
    "            else:\n",
    "                SCROLL_PAUSE_TIME = 1\n",
    "\n",
    "                while (results_number < limit) and continue_scraping:\n",
    "\n",
    "                    if not check_captcha(driver):\n",
    "                        try:\n",
    "                            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                            time.sleep(SCROLL_PAUSE_TIME)\n",
    "                            driver.execute_script(\"return document.body.scrollHeight\") + 400\n",
    "                            page+=1\n",
    "                            search_results+= get_search_results(driver, page)\n",
    "                            results_number = len(search_results)\n",
    "                        except:\n",
    "                            continue_scraping = False\n",
    "                    else:\n",
    "                        continue_scraping = False\n",
    "                        search_results = -1\n",
    "\n",
    "                \n",
    "                driver.quit()    \n",
    "                return search_results\n",
    "\n",
    "\n",
    "        else:\n",
    "            search_results = -1\n",
    "            driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        search_results = -1\n",
    "        return search_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27cd7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test the scraper. it shows all scraped details or an error message, if it fails\n",
    "\n",
    "def test_scraper(query, limit, scraper):\n",
    "    search_results = run(query, limit, scraper)\n",
    "\n",
    "    i = 0\n",
    "    if search_results != -1:\n",
    "        for sr in search_results:\n",
    "            i+=1\n",
    "            print(i)\n",
    "            print(sr[0])\n",
    "            print(sr[1])\n",
    "            print(sr[2])\n",
    "    else:\n",
    "        print(\"Scraping failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12bc2ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise the scraper: Change the parameters for testing your scraper\n",
    "scraper = Scraping() #initialize the scraping object\n",
    "\n",
    "query = \"politik\" #search query\n",
    "limit = 10 #max_number of results (the scraper normally adds some more pages since not all search engines deliver a certain number of search results on every SERP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0389e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Politik\n",
      "Politik bezeichnet die Strukturen (Polity), Prozesse (Politics) und Inhalte (Policy) zur Regelung der Angelegenheiten eines Gemeinwesens – etwa eines ...\n",
      "https://de.wikipedia.org/wiki/Politik\n",
      "2\n",
      "Politik - Aktuelle Nachrichten & News\n",
      "vor 1 Stunde —Nachrichten aus dem In- & Ausland: Politik-News, Reportagen, Analysen, Hintergrundberichte und Interviews aus Deutschland, der Europäischen Union und der ...\n",
      "https://www.welt.de/politik/\n",
      "3\n",
      "Aktuelle Nachrichten aus der Politik auf einen Blick\n",
      "vor 14 Stunden —Politik – Nachrichten, Kommentare und Videos zur Politik in Deutschland und der Welt. Die wichtigsten Informationen zu Wahlen und dem Bundestag.\n",
      "https://www.zdf.de/nachrichten/politik\n",
      "4\n",
      "Politik - DER SPIEGEL\n",
      "vor 14 Stunden —Deutschlands führende Nachrichtenseite. Alles Wichtige aus Politik, Wirtschaft, Sport, Kultur, Wissenschaft, Technik und mehr.\n",
      "https://www.spiegel.de/politik/\n",
      "5\n",
      "Politik | bpb.de\n",
      "Politik. Allg.: P. bezeichnet jegliche Art der Einflussnahme und Gestaltung sowie die Durchsetzung von Forderungen und Zielen, sei es in privaten oder ...\n",
      "https://www.bpb.de/kurz-knapp/lexika/politiklexikon/18019/politik/\n",
      "6\n",
      "Politik: Aktuelle Nachrichten aus dem In- und Ausland\n",
      "vor 6 Stunden —News, Analysen und Hintergrundberichte aus Deutschland und der internationalen Politik. ▻ Bei der FAZ finden Sie aktuelle...\n",
      "https://www.faz.net/aktuell/politik/\n",
      "7\n",
      "Politik | bpb.de\n",
      "Politik ist die „Staatskunst“. Sie regelt das geordnete Zusammenleben der Bürgerinnen und Bürger. Es geht in der Politik also um alles, was mit Gestaltung und ...\n",
      "https://www.bpb.de/kurz-knapp/lexika/das-junge-politik-lexikon/320947/politik/\n",
      "8\n",
      "Politik - aktuelle Nachrichten und Reportagen | ZEIT ONLINE\n",
      "vor 11 Stunden —Aktuelle Nachrichten, Hintergründe und Kommentare zu deutscher und internationaler Politik.\n",
      "https://www.zeit.de/politik/index\n",
      "9\n",
      "Politik-News: Aktuelle Nachrichten aus Deutschland und ...\n",
      "vor 3 Stunden —Aktuelle Politik-News aus Berlin, Deutschland und der Welt. Alle Nachrichten zur Politik aus Inland und Ausland mit Analysen, Hintergründen und Interviews.\n",
      "https://www.tagesspiegel.de/politik/\n",
      "10\n",
      "Politik\n",
      "Politik bezeichnet die Strukturen (Polity), Prozesse (Politics) und Inhalte (Policy) zur Regelung der Angelegenheiten eines Gemeinwesens – etwa eines ...\n",
      "https://de.wikipedia.org/wiki/Politik\n",
      "11\n",
      "Politik - Aktuelle Nachrichten & News\n",
      "vor 1 Stunde —Nachrichten aus dem In- & Ausland: Politik-News, Reportagen, Analysen, Hintergrundberichte und Interviews aus Deutschland, der Europäischen Union und der ...\n",
      "https://www.welt.de/politik/\n",
      "12\n",
      "Aktuelle Nachrichten aus der Politik auf einen Blick\n",
      "vor 14 Stunden —Politik – Nachrichten, Kommentare und Videos zur Politik in Deutschland und der Welt. Die wichtigsten Informationen zu Wahlen und dem Bundestag.\n",
      "https://www.zdf.de/nachrichten/politik\n",
      "13\n",
      "Politik - DER SPIEGEL\n",
      "vor 14 Stunden —Deutschlands führende Nachrichtenseite. Alles Wichtige aus Politik, Wirtschaft, Sport, Kultur, Wissenschaft, Technik und mehr.\n",
      "https://www.spiegel.de/politik/\n",
      "14\n",
      "Politik | bpb.de\n",
      "Politik. Allg.: P. bezeichnet jegliche Art der Einflussnahme und Gestaltung sowie die Durchsetzung von Forderungen und Zielen, sei es in privaten oder ...\n",
      "https://www.bpb.de/kurz-knapp/lexika/politiklexikon/18019/politik/\n",
      "15\n",
      "Politik: Aktuelle Nachrichten aus dem In- und Ausland\n",
      "vor 6 Stunden —News, Analysen und Hintergrundberichte aus Deutschland und der internationalen Politik. ▻ Bei der FAZ finden Sie aktuelle...\n",
      "https://www.faz.net/aktuell/politik/\n",
      "16\n",
      "Politik | bpb.de\n",
      "Politik ist die „Staatskunst“. Sie regelt das geordnete Zusammenleben der Bürgerinnen und Bürger. Es geht in der Politik also um alles, was mit Gestaltung und ...\n",
      "https://www.bpb.de/kurz-knapp/lexika/das-junge-politik-lexikon/320947/politik/\n",
      "17\n",
      "Politik - aktuelle Nachrichten und Reportagen | ZEIT ONLINE\n",
      "vor 11 Stunden —Aktuelle Nachrichten, Hintergründe und Kommentare zu deutscher und internationaler Politik.\n",
      "https://www.zeit.de/politik/index\n",
      "18\n",
      "Politik-News: Aktuelle Nachrichten aus Deutschland und ...\n",
      "vor 3 Stunden —Aktuelle Politik-News aus Berlin, Deutschland und der Welt. Alle Nachrichten zur Politik aus Inland und Ausland mit Analysen, Hintergründen und Interviews.\n",
      "https://www.tagesspiegel.de/politik/\n",
      "19\n",
      "Aktuelle News & Nachrichten aus der Politik\n",
      "vor 5 Stunden —Alle aktuellen News zur Politik aus Deutschland & der Welt auf Merkur.de: Berlin, Wahlen, Ausland, bayerischer Landtag, CSU, München & mehr.\n",
      "https://www.merkur.de/politik/\n",
      "20\n",
      "Politik einfach erklärt: Grundlagen und Zusammenhänge ...\n",
      "Politik beschäftigt sich mit den Entscheidungen und Regeln, die das Zusammenleben von Menschen in einem Staat, wie der Bundesrepublik Deutschland, betreffen und ...\n",
      "https://www.plan.de/themen-einfach-erklaert/politik-einfach-erklaert.html\n",
      "21\n",
      "Politik - News aus Deutschland, Österreich, der Schweiz ...\n",
      "vor 5 Stunden —Politik - Nachrichten aus Deutschland, Österreich, der Schweiz, Europa und der ganzen Welt. News, Hintergründe, Kommentare & Experten-Einschätzungen.\n",
      "https://web.de/magazine/politik/\n",
      "22\n",
      "Politik aktuell - Nachrichten aus Deutschland und der Welt\n",
      "vor 32 Minuten —Das Politische Buch · Das Politische Buch. :Balancieren am Abgrund · Das Politische Buch. :Das Israel, das wir kannten · Das Politische Buch. :Deutsche ...\n",
      "https://www.sueddeutsche.de/politik\n",
      "23\n",
      "Politik für Kinder, einfach erklärt\n",
      "Hallo Uwe, Politik regelt das Zusammenleben der Bürgerinnen und Bürger. Es geht also um alles, was mit der Gestaltung der Gesellschaft und mit Einflussnahme auf ...\n",
      "https://www.hanisauland.de/wissen/lexikon/grosses-lexikon/p/politik.html\n",
      "24\n",
      "Politik aktuell: Nachrichten aus Deutschland, Europa ...\n",
      "Aktuelle News, Informationen und Videos zu Politik, Panorama und Wetter aus Deutschland, Europa und der Welt von t-online.de Nachrichten.\n",
      "https://www.t-online.de/nachrichten/\n",
      "25\n",
      "Politik-Nachrichten aus Deutschland und dem Ausland\n",
      "vor 4 Stunden —Kampf gegen die Klimakrise: Selbst ist der Mensch. Obwohl sich negative Nachrichten rund um die Lage des Weltklimas häufen, glauben die Deutschen weiterhin ...\n",
      "https://www.fr.de/politik/\n",
      "26\n",
      "Aktuelle Politik-Nachrichten\n",
      "vor 5 Stunden —Ideen für eine klimaneutrale Zukunft. Acht Programme hat die Initiative THE MISSION bereits durchgezogen. Das Handelsblatt Research Institute hat sie alle mit ...\n",
      "https://www.handelsblatt.com/politik/\n",
      "27\n",
      "Politik - FOCUS Online - Nachrichten\n",
      "vor 40 Minuten —Wahlen, Krisen und Konflikte: Das Ressort Politik bei FOCUS Online informiert über das politische Geschehen und Politiker in Deutschland und der Welt.\n",
      "https://www.focus.de/politik/\n",
      "28\n",
      "Politik\n",
      "rbb|24 - Nachrichten aus Berlin und Brandenburg über Politik und Gesellschaft.\n",
      "https://www.rbb24.de/politik/\n"
     ]
    }
   ],
   "source": [
    "test_scraper(query, limit, scraper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263056d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
